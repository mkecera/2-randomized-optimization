{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitmlconda3291cc5c44564875b7e62800a0ae8d21",
   "display_name": "Python 3.6.10 64-bit ('ml': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2 - Randomized Optimization - mkecera3@gatech.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
    }
   ],
   "source": [
    "# imports\n",
    "import mlrose\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "save_path = './charts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4-peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 4-peaks problem and print an example\n",
    "fitness = mlrose.FourPeaks(t_pct=0.1)\n",
    "state = np.array([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 100, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(problem, algorithm, **kwargs):\n",
    "    \n",
    "    values = []\n",
    "    names = []\n",
    "    for key, value in kwargs.items():\n",
    "        names.append(key)\n",
    "        values.append(value)\n",
    "    \n",
    "    permutations = [(x, y) for x in values[0] for y in values[1]]\n",
    "    param1 = [x[0] for x in permutations]\n",
    "    param2 = [x[1] for x in permutations]\n",
    "\n",
    "    all_states = []\n",
    "    all_fitness = []\n",
    "    all_curves = []\n",
    "    run_times = []\n",
    "    for permutation in permutations:\n",
    "        start_time = time.time()\n",
    "        param = {}\n",
    "        param[names[0]] = permutation[0]\n",
    "        param[names[1]] = permutation[1]\n",
    "        best_state, best_fitness, curve = algorithm(\n",
    "            problem=problem,\n",
    "            curve=True,\n",
    "            **param\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        \n",
    "        all_states.append(best_state)\n",
    "        all_fitness.append(best_fitness)\n",
    "        run_times.append(run_time)\n",
    "        all_curves.append(curve)\n",
    "\n",
    "    return all_states, all_fitness, all_curves, run_times, param1, param2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 10000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(200, 20000, 8))\n",
    "mut_prob = list(np.linspace(0, 0.5, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "mut_prob = [round(x, 2) for x in mut_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, mut_prob_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    mutation_prob=mut_prob,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(50, 500, 8))\n",
    "keep_pct = list(np.linspace(0.1, 0.4, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "keep_pct = [round(x, 2) for x in keep_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, keep_perc_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    keep_pct=keep_pct,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Keep Percantage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Keep Percentage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Knapsack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Knapsack problem and print an example\n",
    "value = [1, 5, 10, 20, 5, 1, 10, 5, 8]\n",
    "weight = [5, 5, 15, 7, 12, 10, 20, 1, 7]\n",
    "max_weight = 0.6\n",
    "fitness = mlrose.Knapsack(weights=weight, values=value, max_weight_pct=max_weight)\n",
    "state = np.array([1, 0, 1, 0, 1, 0, 0, 1, 0])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 9, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 10000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(200, 20000, 8))\n",
    "mut_prob = list(np.linspace(0, 0.5, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "mut_prob = [round(x, 2) for x in mut_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, mut_prob_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    mutation_prob=mut_prob,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(500, 500, 8))\n",
    "keep_pct = list(np.linspace(0.1, 0.4, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "keep_pct = [round(x, 2) for x in keep_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, keep_perc_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    keep_pct=keep_pct,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Keep Percantage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Keep Percentage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Count ones problem and print an example\n",
    "fitness = mlrose.OneMax()\n",
    "state = np.random.choice([0, 1], size=100, p=[0.5, 0.5])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 100, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 10000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, mut_prob_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    mutation_prob=mut_prob,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Mutation Prob': mut_prob_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Mutation Prob', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(50, 500, 8))\n",
    "keep_pct = list(np.linspace(0.1, 0.4, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "keep_pct = [round(x, 2) for x in keep_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, keep_perc_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    keep_pct=keep_pct,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Keep Percantage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Keep Percentage': keep_perc_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Keep Percentage', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "creditDataDf = pd.read_excel('./data/default of credit card clients.xls', header=1)\n",
    "creditDataDf = creditDataDf.drop(columns=['ID'])\n",
    "\n",
    "# create dummy variables from categorical\n",
    "creditDataDf = pd.get_dummies(creditDataDf, prefix=['SEX', 'EDUCATION', 'MARRIAGE'], columns=['SEX', 'EDUCATION', 'MARRIAGE'])\n",
    "\n",
    "# drop last mummy variable\n",
    "creditDataDf = creditDataDf.drop(columns=['SEX_2', 'EDUCATION_6', 'MARRIAGE_3'])\n",
    "\n",
    "y = creditDataDf['default payment next month']\n",
    "creditDataDf = creditDataDf.drop(columns=['default payment next month'])\n",
    "dfColumns = creditDataDf.columns\n",
    "\n",
    "# balance the dataset\n",
    "ros = RandomOverSampler(random_state=37)\n",
    "creditDataDf, y = ros.fit_resample(creditDataDf, y)\n",
    "\n",
    "creditDataDf = pd.DataFrame(creditDataDf, columns=dfColumns)\n",
    "\n",
    "# normalize the data\n",
    "x = creditDataDf.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "creditDataDf = pd.DataFrame(x_scaled, columns=creditDataDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(creditDataDf, y, test_size=0.2, random_state=37)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "print(len(X_train.columns))\n",
    "print(sum(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was adapted from https://mlrose.readthedocs.io/en/stable/source/tutorial3.html\n",
    "# Initialize neural network object and fit object\n",
    "nn_model = mlrose.NeuralNetwork(\n",
    "    hidden_nodes = [25, 15], \n",
    "    activation = 'relu',\n",
    "    algorithm = 'gradient_descent', \n",
    "    max_iters = 10000,\n",
    "    bias = False, \n",
    "    is_classifier = True, \n",
    "    learning_rate = 0.0001,\n",
    "    early_stopping = True, \n",
    "    clip_max = 50, \n",
    "    max_attempts = 1000,\n",
    "    random_state = 3,\n",
    "    restarts=10\n",
    "    )\n",
    "\n",
    "nn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for train set and assess accuracy\n",
    "y_train_pred = nn_model.predict(X_train)\n",
    "y_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(y_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "X_val_tensor = torch.tensor(X_val.values.astype(np.float32))\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.int))\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.int))\n",
    "y_val_tensor = torch.tensor(y_val.values.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTorchDataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valTorchDataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "testTorchDataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valTorchDataset), len(testTorchDataset), len(trainTorchDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader2 = torch.utils.data.DataLoader(trainTorchDataset, batch_size=64, shuffle=True)\n",
    "valloader2 = torch.utils.data.DataLoader(valTorchDataset, batch_size=len(valTorchDataset), shuffle=True)\n",
    "testloader2 = torch.utils.data.DataLoader(testTorchDataset, batch_size=len(testTorchDataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredictTabular(model2, valloader2, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    probList = []\n",
    "    for rows,labels in valloader2:\n",
    "        \n",
    "#         print(rows.shape)\n",
    "#         print(labels.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(rows.float())\n",
    "            output = model2(rows)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            row = rows[i].view(1, 30)\n",
    "#             print(row.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model2(row)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            probList.append(probab)\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader2)))\n",
    "    \n",
    "    return running_loss/len(valloader2), correct_count, all_count, probList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_sizes = [2]\n",
    "output_size = 2\n",
    "\n",
    "modelTabBaseline = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    # nn.ReLU(),\n",
    "    # nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    # nn.ReLU(),\n",
    "    # nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTabNN(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    time0 = time.time()\n",
    "    epochs = 100\n",
    "    lossData = []\n",
    "    accData=[]\n",
    "    runTime = []\n",
    "    for e in range(epochs):\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "        for rows, labels in trainloader2:\n",
    "                \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(rows)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader2)))\n",
    "            \n",
    "        testingLoss, correctCount, allCount, proba = nnPredictTabular(model, valloader2, criterion)\n",
    "        print(\"Testing Loss  =\", (testingLoss))\n",
    "        print(\"Testing Accuracy  =\", (correctCount / allCount))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader2), 'Training Loss'])\n",
    "        lossData.append([e, testingLoss, 'Testing Loss'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "    \n",
    "    return lossData, accData, runTime\n",
    "        \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_fitness_function(state, model):\n",
    "    time0 = time.time()\n",
    "    lossData = []\n",
    "    accData=[]\n",
    "    runTime = []\n",
    "    startTime = time.time()\n",
    "\n",
    "    l1 = torch.tensor(np.reshape(state[0:60], (2, 30)).astype(np.float32))\n",
    "    # l2 = torch.tensor(np.reshape(state[750:1125], (15, 25)).astype(np.float32))\n",
    "    # l3 = torch.tensor(np.reshape(state[1125:1155], (2, 15)).astype(np.float32))\n",
    "    # print(model[4].weight)\n",
    "    # print(torch.nn.Parameter(l3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model[0].weight = torch.nn.Parameter(l1)\n",
    "        # model[2].weight = torch.nn.Parameter(l2)\n",
    "        # model[4].weight = torch.nn.Parameter(l3)\n",
    "        \n",
    "    testingLoss, correctCount, allCount, proba = nnPredictTabular(model, trainloader2, criterion)\n",
    "    print(\"Loss  =\", (testingLoss))\n",
    "    # print(\"Accuracy  =\", (correctCount / allCount))\n",
    "    \n",
    "    # lossData.append([e, running_loss/len(trainloader2), 'Training Loss'])\n",
    "    # lossData.append([e, testingLoss, 'Testing Loss'])\n",
    "    # accData.append([e, correctCount / allCount])\n",
    "\n",
    "    # runTime.append(time.time() - startTime)\n",
    "    \n",
    "    return testingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnessNN = mlrose.CustomFitness(NN_fitness_function, model=modelTabBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = list(np.random.uniform(-0.1, 0.1, 60))\n",
    "fitnessNN.evaluate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = mlrose.ContinuousOpt(\n",
    "    length = 60, \n",
    "    fitness_fn = fitnessNN, \n",
    "    maximize = False, \n",
    "    min_val=-0.1, \n",
    "    max_val = 0.1,\n",
    "    step=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decay schedule\n",
    "schedule = mlrose.ExpDecay()\n",
    "\n",
    "best_state, best_fitness = mlrose.random_hill_climb(\n",
    "    problem, \n",
    "    # schedule = schedule,\n",
    "    max_attempts = 10, \n",
    "    max_iters = 1000,\n",
    "    init_state = state, \n",
    "    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline, runtime = trainTabNN(modelTabBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount, proba = nnPredictTabular(modelTabBaseline, testloader2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount/allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataBaseline = [[row[0], row[1], 'Baseline Accuracy'] for row in accDataBaseline]\n",
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataBaseline = pd.DataFrame.from_records(accDataBaseline, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns_plot = sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('NN')\n",
    "# sns_plot.get_figure().savefig(savePath + \"NN_CREDIT_baseline_learning-curve.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "    \n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataBaseline\n",
    "    ).set_title('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-edaaedd57bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([transforms.ToTensor(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               ])\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# transform = transforms.Compose([transforms.ToTensor()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(trainset, replacement=True, num_samples=5000)\n",
    "val_sampler = torch.utils.data.sampler.RandomSampler(valset, replacement=True, num_samples=5000)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(model, valloader, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    for images,labels in valloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "#             print(img.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader)))\n",
    "    \n",
    "    return running_loss/len(valloader), correct_count, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    time0 = time.time()\n",
    "    epochs = 30\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    runTime = []\n",
    "    for e in range(epochs):\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "        \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "            \n",
    "        testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader), 'Training Error'])\n",
    "        lossData.append([e, testingLoss, 'Testing Error'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "    \n",
    "    return lossData, accData, runTime    \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_randomized_hill_climbing(model, epochs):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    time0 = time.time()\n",
    "    epochs = epochs\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    runTime = []\n",
    "    previous_loss = 999\n",
    "\n",
    "    for e in range(epochs):\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        state = np.random.normal(0, 0.01, 109184)\n",
    "\n",
    "        l1 = torch.tensor(np.reshape(state[0:100352], (128, 784)).astype(np.float32))\n",
    "        l2 = torch.tensor(np.reshape(state[100352:108544], (64, 128)).astype(np.float32))\n",
    "        l3 = torch.tensor(np.reshape(state[108544:109184], (10, 64)).astype(np.float32))\n",
    "        # print(l1.shape)\n",
    "        # print(torch.nn.Parameter(l3))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_w1 = model[0].weight\n",
    "            orig_w2 = model[2].weight\n",
    "            orig_w3 = model[4].weight\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model[0].weight = torch.nn.Parameter(model[0].weight + l1)\n",
    "            model[2].weight = torch.nn.Parameter(model[2].weight + l2)\n",
    "            model[4].weight = torch.nn.Parameter(model[4].weight + l3)\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "            # # Training pass\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            # output = model(images)\n",
    "            # loss = criterion(output, labels)\n",
    "            \n",
    "            # #This is where the model learns by backpropagating\n",
    "            # loss.backward()\n",
    "            \n",
    "            # #And optimizes its weights here\n",
    "            # optimizer.step()\n",
    "            \n",
    "            # running_loss += loss.item()\n",
    "        else:\n",
    "            current_loss = running_loss/len(trainloader)\n",
    "            # print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "\n",
    "        print('Epoch: ' + str(e), 'Previous loss: ' + str(previous_loss), 'Current loss: ' + str(current_loss))\n",
    "        if current_loss > previous_loss:\n",
    "            with torch.no_grad():\n",
    "                model[0].weight = orig_w1\n",
    "                model[2].weight = orig_w2\n",
    "                model[4].weight = orig_w3\n",
    "        else:\n",
    "            previous_loss = current_loss\n",
    "            \n",
    "        testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        # print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader), 'Training Error'])\n",
    "        lossData.append([e, testingLoss, 'Testing Error'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "\n",
    "        # print(model[0].weight)\n",
    "    \n",
    "    return lossData, accData, runTime    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline, runtime = trainNN_randomized_hill_climbing(model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataBaseline = [[row[0], row[1], 'Baseline Accuracy'] for row in accDataBaseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataBaseline = pd.DataFrame.from_records(accDataBaseline, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "# sns.lineplot(\n",
    "#     x=\"Epoch\", y=\"Loss\",\n",
    "#     hue=\"Stage\",\n",
    "#     data=lossDataBaseline\n",
    "#     ).set_title('Baseline NN')\n",
    "\n",
    "sns_plot = sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('Baseline NN')\n",
    "# sns_plot.get_figure().savefig(savePath + \"NN_MNIST_baseline_learning-curve.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataBaseline\n",
    "    ).set_title('3 Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_fitness_function(state, model):\n",
    "    time0 = time.time()\n",
    "    lossData = []\n",
    "    accData=[]\n",
    "    runTime = []\n",
    "    startTime = time.time()\n",
    "\n",
    "    l1 = torch.tensor(np.reshape(state[0:100352], (128, 784)).astype(np.float32))\n",
    "    l2 = torch.tensor(np.reshape(state[100352:108544], (64, 128)).astype(np.float32))\n",
    "    l3 = torch.tensor(np.reshape(state[108544:109184], (10, 64)).astype(np.float32))\n",
    "    # print(l1.shape)\n",
    "    # print(torch.nn.Parameter(l3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model[0].weight = torch.nn.Parameter(l1)\n",
    "        model[2].weight = torch.nn.Parameter(l2)\n",
    "        model[4].weight = torch.nn.Parameter(l3)\n",
    "        \n",
    "    testingLoss, correctCount, allCount = nnPredict(model, trainloader, criterion)\n",
    "    print(\"Loss  =\", (testingLoss))\n",
    "    # print(\"Accuracy  =\", (correctCount / allCount))\n",
    "    \n",
    "    # lossData.append([e, running_loss/len(trainloader2), 'Training Loss'])\n",
    "    # lossData.append([e, testingLoss, 'Testing Loss'])\n",
    "    # accData.append([e, correctCount / allCount])\n",
    "\n",
    "    # runTime.append(time.time() - startTime)\n",
    "    \n",
    "    return testingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnessNN = mlrose.CustomFitness(NN_fitness_function, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = list(np.random.uniform(-0.1, 0.1, 109184))\n",
    "fitnessNN.evaluate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = mlrose.ContinuousOpt(\n",
    "    length = 109184, \n",
    "    fitness_fn = fitnessNN, \n",
    "    maximize = False, \n",
    "    min_val=-0.1, \n",
    "    max_val = 0.1,\n",
    "    step=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decay schedule\n",
    "schedule = mlrose.ExpDecay()\n",
    "\n",
    "best_state, best_fitness = mlrose.random_hill_climb(\n",
    "    problem, \n",
    "    # schedule = schedule,\n",
    "    max_attempts = 10, \n",
    "    max_iters = 3000,\n",
    "    init_state = state,\n",
    "    restarts = 20, \n",
    "    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('./data', download=False, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=False, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MNIST = trainset.data.numpy()\n",
    "X_test_MNIST = testset.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_MNIST = trainset.targets.numpy()\n",
    "y_test_MNIST = testset.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MNIST = X_train_MNIST[0:4000]\n",
    "X_test_MNIST = X_test_MNIST[0:4000]\n",
    "y_train_MNIST = y_train_MNIST[0:4000]\n",
    "y_test_MNIST = y_test_MNIST[0:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train_MNIST) + np.bincount(y_test_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MNIST_reshaped = X_train_MNIST.reshape((4000, 784))\n",
    "X_test_MNIST_reshaped = X_test_MNIST.reshape((4000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_MNIST_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_MNIST_reshaped)\n",
    "\n",
    "# One hot encode target values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(y_train_MNIST.reshape(-1, 1)).todense()\n",
    "y_test_hot = one_hot.transform(y_test_MNIST.reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was adapted from https://mlrose.readthedocs.io/en/stable/source/tutorial3.html\n",
    "# Initialize neural network object and fit object\n",
    "nn_model = mlrose.NeuralNetwork(\n",
    "    hidden_nodes = [100], \n",
    "    activation = 'relu',\n",
    "    algorithm = 'random_hill_climb', \n",
    "    max_iters = 1000,\n",
    "    bias = True, \n",
    "    is_classifier = True, \n",
    "    learning_rate = 0.001,\n",
    "    early_stopping = True, \n",
    "    clip_max = 5, \n",
    "    max_attempts = 100,\n",
    "    random_state = 3,\n",
    "    restarts=20\n",
    "    )\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for train set and assess accuracy\n",
    "y_train_pred = nn_model.predict(X_train_scaled)\n",
    "y_train_accuracy = accuracy_score(y_train_pred, y_train_hot)\n",
    "print(y_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for train set and assess accuracy\n",
    "y_test_pred = nn_model.predict(X_test_scaled)\n",
    "y_test_accuracy = accuracy_score(y_test_pred, y_test_hot)\n",
    "print(y_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}