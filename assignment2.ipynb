{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Randomized Optimization - mkecera3@gatech.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mlrose\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "save_path = './charts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import random\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 4-peaks problem and print an example\n",
    "fitness = mlrose.FourPeaks(t_pct=0.1)\n",
    "state = np.array([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 100, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(problem, algorithm, **kwargs):\n",
    "    \n",
    "    values = []\n",
    "    names = []\n",
    "    for key, value in kwargs.items():\n",
    "        names.append(key)\n",
    "        values.append(value)\n",
    "    \n",
    "    permutations = [(x, y) for x in values[0] for y in values[1]]\n",
    "    param1 = [x[0] for x in permutations]\n",
    "    param2 = [x[1] for x in permutations]\n",
    "\n",
    "    all_states = []\n",
    "    all_fitness = []\n",
    "    all_curves = []\n",
    "    run_times = []\n",
    "    for permutation in permutations:\n",
    "        start_time = time.time()\n",
    "        param = {}\n",
    "        param[names[0]] = permutation[0]\n",
    "        param[names[1]] = permutation[1]\n",
    "        best_state, best_fitness, curve = algorithm(\n",
    "            problem=problem,\n",
    "            curve=True,\n",
    "            **param\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        \n",
    "        all_states.append(best_state)\n",
    "        all_fitness.append(best_fitness)\n",
    "        run_times.append(run_time)\n",
    "        all_curves.append(curve)\n",
    "\n",
    "    return all_states, all_fitness, all_curves, run_times, param1, param2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_rhc_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 20000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_sa_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(200, 20000, 8))\n",
    "max_iter = list(np.geomspace(50, 5000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_ga_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(50, 500, 8))\n",
    "max_iter = list(np.geomspace(50, 2000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"4p_mimic_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knapsack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Knapsack problem and print an example\n",
    "value = [1, 5, 10, 20, 5, 1, 10, 5, 8]\n",
    "weight = [5, 5, 15, 7, 12, 10, 20, 1, 7]\n",
    "max_weight = 0.6\n",
    "fitness = mlrose.Knapsack(weights=weight, values=value, max_weight_pct=max_weight)\n",
    "state = np.array([1, 0, 2, 0, 1, 0, 0, 1, 0])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 9, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_rhc_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 20000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_sa_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(200, 20000, 8))\n",
    "max_iter = list(np.geomspace(50, 5000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_ga_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(50, 500, 8))\n",
    "max_iter = list(np.geomspace(50, 2000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"ks_mimic_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Count ones problem and print an example\n",
    "fitness = mlrose.OneMax()\n",
    "state = np.random.choice([0, 1], size=100, p=[0.5, 0.5])\n",
    "print(fitness.evaluate(state))\n",
    "problem = mlrose.DiscreteOpt(length = 100, fitness_fn = fitness, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the testing\n",
    "restart_options = list(np.geomspace(10, 1000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 500, 8))\n",
    "restart_options = [int(x) for x in restart_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, restart_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.random_hill_climb,\n",
    "    restarts=restart_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_rhc_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Restarts': restart_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Restarts', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Randomized Hill Climbing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_rhc_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_options = list(np.geomspace(10, 20000, 8))\n",
    "max_attempts_options = list(np.geomspace(10, 10000, 8))\n",
    "iters_options = [int(x) for x in iters_options]\n",
    "max_attempts_options = [int(x) for x in max_attempts_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, max_iter_data, max_attempt_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.simulated_annealing,\n",
    "    max_iters=iters_options, \n",
    "    max_attempts=max_attempts_options,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_sa_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Max Iterations': max_iter_data, 'Max Attempts': max_attempt_data})\n",
    "chart_data = chart_data.pivot(index='Max Iterations', columns='Max Attempts', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Simulated Annealing - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_sa_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(200, 20000, 8))\n",
    "max_iter = list(np.geomspace(50, 5000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.genetic_alg,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_ga_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('Genetic Algorithms - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_ga_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = list(np.geomspace(50, 500, 8))\n",
    "max_iter = list(np.geomspace(50, 2000, 8))\n",
    "pop_size = [int(x) for x in pop_size]\n",
    "max_iter = [int(x) for x in max_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states, all_fitness, all_curves, run_times, pop_size_data, max_iter_data = parameter_search(\n",
    "    problem=problem, \n",
    "    algorithm=mlrose.mimic,\n",
    "    pop_size=pop_size, \n",
    "    max_iters=max_iter,\n",
    "    curve=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Fitness': all_fitness, 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Fitness')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Fitness')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_mimic_hyperparam.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame({'Run Times': [round(x, 2) for x in run_times], 'Population Size': pop_size_data, 'Max Iterations': max_iter_data})\n",
    "chart_data = chart_data.pivot(index='Population Size', columns='Max Iterations', values='Run Times')\n",
    "sns.set_context(\"paper\")\n",
    "sns_plot = sns.heatmap(chart_data, annot=True, fmt=\"g\", cmap='viridis').set_title('MIMIC - Run Times')\n",
    "sns_plot.get_figure().savefig(save_path + \"co_mimic_hyperparam-time.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(trainset, replacement=True, num_samples=5000)\n",
    "val_sampler = torch.utils.data.sampler.RandomSampler(valset, replacement=True, num_samples=5000)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(model, valloader, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    for images,labels in valloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "#             print(img.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader)))\n",
    "    \n",
    "    return running_loss/len(valloader), correct_count, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    time0 = time.time()\n",
    "    epochs = 30\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    runTime = []\n",
    "    for e in range(epochs):\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "        \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "            \n",
    "        # testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        # print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader), 'Training Error'])\n",
    "        lossData.append([e, testingLoss, 'Testing Error'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "    \n",
    "    return lossData, accData, runTime    \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_randomized_hill_climbing(model, stdev, restarts, max_iter):\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    time0 = time.time()\n",
    "    # epochs = epochs\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    runTime = []\n",
    "    previous_loss = 999\n",
    "    run = True\n",
    "    iteration = 0\n",
    "    restarted = 0\n",
    "    total_iterations = 0\n",
    "    update_ls = True\n",
    "    best_loss_across = 999\n",
    "\n",
    "    best_w1 = model[0].weight\n",
    "    best_w2 = model[2].weight\n",
    "    best_w3 = model[4].weight\n",
    "\n",
    "    while run:\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        state = np.random.normal(0, stdev, 109184)\n",
    "\n",
    "        l1 = torch.tensor(np.reshape(state[0:100352], (128, 784)).astype(np.float32))\n",
    "        l2 = torch.tensor(np.reshape(state[100352:108544], (64, 128)).astype(np.float32))\n",
    "        l3 = torch.tensor(np.reshape(state[108544:109184], (10, 64)).astype(np.float32))\n",
    "        # print(l1.shape)\n",
    "        # print(torch.nn.Parameter(l3))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_w1 = model[0].weight\n",
    "            orig_w2 = model[2].weight\n",
    "            orig_w3 = model[4].weight\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model[0].weight = torch.nn.Parameter(model[0].weight + l1)\n",
    "            model[2].weight = torch.nn.Parameter(model[2].weight + l2)\n",
    "            model[4].weight = torch.nn.Parameter(model[4].weight + l3)\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        else:\n",
    "            current_loss = running_loss/len(trainloader)\n",
    "            # print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "\n",
    "        print(\n",
    "            'Restarted: ' + str(restarted),\n",
    "            'Iterations w/o improvement: ' + str(iteration), \n",
    "            'Previous loss: ' + str(round(previous_loss, 3)), \n",
    "            'Current loss: ' + str(round(current_loss, 3)),\n",
    "            'Best loss: ' + str(round(best_loss_across, 3))\n",
    "            )\n",
    "        \n",
    "        if current_loss > previous_loss:\n",
    "            iteration += 1\n",
    "            with torch.no_grad():\n",
    "                model[0].weight = orig_w1\n",
    "                model[2].weight = orig_w2\n",
    "                model[4].weight = orig_w3\n",
    "            \n",
    "            if iteration > max_iter:\n",
    "\n",
    "                restarted += 1\n",
    "                previous_loss = 999\n",
    "\n",
    "                model = nn.Sequential(\n",
    "                    nn.Linear(input_size, hidden_sizes[0]),\n",
    "                    nn.ReLU(),\n",
    "                    # nn.Dropout(drop_out, inplace=True),\n",
    "                    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                    nn.ReLU(),\n",
    "                    # nn.Dropout(drop_out, inplace=True),\n",
    "                    nn.Linear(hidden_sizes[1], output_size),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                    )\n",
    "\n",
    "                iteration = 0\n",
    "\n",
    "                if restarted > restarts:\n",
    "                    run = False\n",
    "                    model[0].weight = best_w1\n",
    "                    model[2].weight = best_w2\n",
    "                    model[4].weight = best_w3\n",
    "        else:\n",
    "            previous_loss = current_loss\n",
    "            if current_loss < best_loss_across:\n",
    "                best_w1 = model[0].weight\n",
    "                best_w2 = model[2].weight\n",
    "                best_w3 = model[4].weight\n",
    "                best_loss_across = current_loss\n",
    "            \n",
    "            \n",
    "        # testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        # print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([total_iterations, running_loss/len(trainloader), 'Training Error'])\n",
    "        # lossData.append([total_iterations, testingLoss, 'Testing Error'])\n",
    "        # accData.append([total_iterations, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "        total_iterations += 1\n",
    "\n",
    "        # print(model[0].weight)\n",
    "        \n",
    "    model[0].weight = best_w1\n",
    "    model[2].weight = best_w2\n",
    "    model[4].weight = best_w3\n",
    "    \n",
    "    return lossData, accData, runTime    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline, runtime = trainNN_randomized_hill_climbing(model, 0.01, 5, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "# sns.lineplot(\n",
    "#     x=\"Epoch\", y=\"Loss\",\n",
    "#     hue=\"Stage\",\n",
    "#     data=lossDataBaseline\n",
    "#     ).set_title('Baseline NN')\n",
    "\n",
    "sns_plot = sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('Randomized Hill Climbing')\n",
    "sns_plot.get_figure().savefig(save_path + \"nn-randomized-hill.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount, testingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(runtime) * len(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(model, valloader, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    for images,labels in valloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "#             print(img.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader)))\n",
    "    \n",
    "    return running_loss/len(valloader), correct_count, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_simulated_annealing(model, stdev, max_iter, T, decay):\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    time0 = time.time()\n",
    "    # epochs = epochs\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    runTime = []\n",
    "    previous_loss = 999\n",
    "    total_iterations = 0\n",
    "    update_ls = True\n",
    "    best_loss_across = 999\n",
    "\n",
    "    best_w1 = model[0].weight\n",
    "    best_w2 = model[2].weight\n",
    "    best_w3 = model[4].weight\n",
    "\n",
    "    for e in range(0, max_iter):\n",
    "        startTime = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        # state = np.random.normal(0, stdev, 109184)\n",
    "\n",
    "        l1 = torch.tensor(np.random.normal(0, stdev, (128, 784)).astype(np.float32))\n",
    "        l2 = torch.tensor(np.random.normal(0, stdev, (64, 128)).astype(np.float32))\n",
    "        l3 = torch.tensor(np.random.normal(0, stdev, (10, 64)).astype(np.float32))\n",
    "        # print(l1.shape)\n",
    "        # print(torch.nn.Parameter(l3))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_w1 = model[0].weight\n",
    "            orig_w2 = model[2].weight\n",
    "            orig_w3 = model[4].weight\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model[0].weight = torch.nn.Parameter(model[0].weight + l1)\n",
    "            model[2].weight = torch.nn.Parameter(model[2].weight + l2)\n",
    "            model[4].weight = torch.nn.Parameter(model[4].weight + l3)\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        else:\n",
    "            current_loss = running_loss/len(trainloader)\n",
    "            # print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "\n",
    "        print(\n",
    "            'Iteration: ' + str(e), \n",
    "            'Previous loss: ' + str(round(previous_loss, 3)), \n",
    "            'Current loss: ' + str(round(current_loss, 3)),\n",
    "            )\n",
    "        \n",
    "        if current_loss > previous_loss:\n",
    "            loss_difference = current_loss - previous_loss\n",
    "            prob_accept = np.exp(-loss_difference/(T))\n",
    "            prob_uniform = np.random.uniform()\n",
    "            # print('Loss difference: ' + str(loss_difference))\n",
    "            # print('Prob of accepting the move: ' + str(prob_accept))\n",
    "            # print('Prob hurdle: ' + str(prob_uniform))\n",
    "            \n",
    "            if prob_accept > prob_uniform:\n",
    "                # print('Accepted worse solution')\n",
    "                previous_loss = current_loss\n",
    "\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    model[0].weight = orig_w1\n",
    "                    model[2].weight = orig_w2\n",
    "                    model[4].weight = orig_w3\n",
    "\n",
    "        else:\n",
    "            previous_loss = current_loss\n",
    "            # best_w1 = model[0].weight\n",
    "            # best_w2 = model[2].weight\n",
    "            # best_w3 = model[4].weight\n",
    "            \n",
    "            \n",
    "        # testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        # print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader), 'Training Error'])\n",
    "        # lossData.append([e, testingLoss, 'Testing Error'])\n",
    "        # accData.append([e, correctCount / allCount])\n",
    "\n",
    "        runTime.append(time.time() - startTime)\n",
    "\n",
    "        T *= decay\n",
    "        # print('New temp: ' + str(T))\n",
    "\n",
    "        # print(model[0].weight)\n",
    "    \n",
    "    return lossData, accData, runTime    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline, runtime = trainNN_simulated_annealing(\n",
    "    model, \n",
    "    stdev=0.01, \n",
    "    max_iter=50000, \n",
    "    T=0.01, \n",
    "    decay=0.9999\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accDataBaseline = [[row[0], row[1], 'Baseline Accuracy'] for row in accDataBaseline]\n",
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "# accDataBaseline = pd.DataFrame.from_records(accDataBaseline, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns_plot = sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('Simulated Annealing')\n",
    "sns_plot.get_figure().savefig(save_path + \"nn-simulated-annealing.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount, testingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(runtime) * len(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(model, valloader, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    for images,labels in valloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "#             print(img.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader)))\n",
    "    \n",
    "    return running_loss/len(valloader), correct_count, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_GA(individual):\n",
    "   \n",
    "    running_loss = 0\n",
    "\n",
    "    a = individual[0:100352]\n",
    "    b = individual[100352:108544]\n",
    "    c = individual[108544:109184]\n",
    "\n",
    "    l1 = torch.tensor(np.reshape(a, (128, 784)).astype(np.float32))\n",
    "    l2 = torch.tensor(np.reshape(b, (64, 128)).astype(np.float32))\n",
    "    l3 = torch.tensor(np.reshape(c, (10, 64)).astype(np.float32))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model[0].weight = torch.nn.Parameter(l1)\n",
    "        model[2].weight = torch.nn.Parameter(l2)\n",
    "        model[4].weight = torch.nn.Parameter(l3)\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss/len(trainloader),    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was adpated from https://deap.readthedocs.io/en/master/examples/ga_onemax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(model):\n",
    "\n",
    "    data = model.state_dict()\n",
    "\n",
    "    l1 = list(np.reshape(np.array(data['0.weight']), 100352))\n",
    "    l2 = list(np.reshape(np.array(data['2.weight']), 8192))\n",
    "    l3 = list(np.reshape(np.array(data['4.weight']), 640))\n",
    "\n",
    "    a = l1 + l2 + l3\n",
    "\n",
    "    # print(len(a))\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generator \n",
    "# toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"get_weights\", model_weights, model)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\n",
    "    \"individual\", \n",
    "    tools.initIterate, \n",
    "    creator.Individual, \n",
    "    toolbox.get_weights \n",
    "    )\n",
    "toolbox.register(\n",
    "    \"population\", \n",
    "    tools.initRepeat, \n",
    "    list, \n",
    "    toolbox.individual\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox.register(\"evaluate\", trainNN_GA)\n",
    "toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, indpb=0.05, mu=0.0, sigma=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga():\n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.4, mutpb=0.2, ngen=1000, \n",
    "                                   stats=stats, halloffame=hof, verbose=True)\n",
    "    \n",
    "    return pop, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "pop, log = ga()\n",
    "runTime = time.time() - time0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns_plot = sns.lineplot(\n",
    "    x=\"gen\", y=\"avg\",\n",
    "    data=data\n",
    "    ).set_title('Genetic Algorithm')\n",
    "sns_plot.get_figure().savefig(save_path + \"ga-nn.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount, testingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ml': conda)",
   "language": "python",
   "name": "python361064bitmlconda3291cc5c44564875b7e62800a0ae8d21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
